{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from pingouin import partial_corr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import math, joblib, os\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, timedelta\n",
    "from .models import Actuals, Forecasts, Pracpredictions\n",
    "from django.conf import settings\n",
    "import json\n",
    "\n",
    "class ElectricityForecaster:\n",
    "    \n",
    "    NUM_OF_STEPS = 48\n",
    "    NUM_OF_MODELS = 48\n",
    "    NUM_OF_LAGS = 168\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self.path =  os.path.dirname(os.path.abspath(__file__))\n",
    "        self.exog = self.read_file(\"exog.txt\")\n",
    "        self.models = [xgb.XGBRegressor()  for _ in range(self.NUM_OF_MODELS)]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit_and_predict(self, data):\n",
    "        data = self.transform_data_valid(data)\n",
    "        train, valid = self.split_data(data)\n",
    "        self.fit(train)\n",
    "        preds, actuals, mae, mape, rmse = self.predict_validation(valid)\n",
    "        return preds, actuals, mae, mape, rmse\n",
    "    \n",
    "\n",
    "    def predict_validation(self, data, transformed=True):\n",
    "        if not transformed:\n",
    "            data = self.transform_data(data)\n",
    "        aes, apes, ses = [], [], []  \n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for i in range(self.NUM_OF_MODELS):\n",
    "\n",
    "            x_columns = self.x_cols(i)\n",
    "            y_columns = self.y_cols(i)\n",
    "\n",
    "            valid_x = data[x_columns]\n",
    "            valid_y = data[y_columns].values\n",
    "\n",
    "            current_model = self.models[i]\n",
    "            prediction = current_model.predict(valid_x)\n",
    "\n",
    "            aes += list(self.absolute_errors(valid_y, prediction))\n",
    "            apes += list(self.absolute_perc_errors(valid_y, prediction))\n",
    "            ses += list((self.squared_errors(valid_y, prediction)))\n",
    "            \n",
    "            preds += prediction.tolist()\n",
    "            actuals += list(valid_y)\n",
    "            \n",
    "        mae = self.get_mae(aes)\n",
    "        mape = self.get_mape(apes)\n",
    "        rmse = self.get_rmse(ses)\n",
    "        \n",
    "        print(f\"mae: {mae}\")\n",
    "        print(f\"mape: {mape}\")\n",
    "        print(f\"rmse: {rmse}\")\n",
    "        \n",
    "        return preds,actuals,  mae, mape, rmse\n",
    "    \n",
    "\n",
    "    def predict_multiple(self, data, transformed = False):\n",
    "        aes, apes, ses = [], [], []  \n",
    "        preds = []\n",
    "        actuals = []\n",
    "        errors = []\n",
    "        if not transformed:\n",
    "            data = self.transform_data(data)\n",
    "        smaller_dfs = [data[i:i + 1] for i in range(0, len(data))]\n",
    "        for i in range(len(smaller_dfs)):\n",
    "             \n",
    "            row = smaller_dfs[i]\n",
    "            \n",
    "            print(row)\n",
    "            \n",
    "            prediction, actual, ae, ape, se = self.predict_48_hours_validation(row, False)\n",
    "            aes += ae\n",
    "            apes += ape\n",
    "            ses += se\n",
    "            preds += prediction\n",
    "            actuals += actual\n",
    "            \n",
    "            start_datetime = row.index.to_pydatetime()[0]\n",
    "            end_datetime = start_datetime + timedelta(hours= self.NUM_OF_STEPS -1 )\n",
    "            new_row = [start_datetime, end_datetime, self.get_mae(ae), self.get_mape(ape), self.get_rmse(aes)]\n",
    "            errors.append(new_row)\n",
    "            print(start_datetime)\n",
    "            print(new_row)\n",
    "            print(ae)\n",
    "            print(ape)\n",
    "            print(se)\n",
    "            print(\"\\n\\n\\n\")\n",
    "\n",
    "            \n",
    "        mae = self.get_mae(aes)\n",
    "        mape = self.get_mape(apes)\n",
    "        rmse = self.get_rmse(ses)\n",
    "        \n",
    "        errors = pd.DataFrame(errors, columns=['start_datetime', 'end_datetime', 'mae', 'mape', 'rmse'])\n",
    "        \n",
    "        print(f\"mae: {mae}\")\n",
    "        print(f\"mape: {mape}\")\n",
    "        print(f\"rmse: {rmse}\")\n",
    "        \n",
    "        return preds,actuals,  mae, mape, rmse, errors\n",
    "    \n",
    "\n",
    "    def predict(self, data, transformed = False):\n",
    "        preds = []\n",
    "\n",
    "        smaller_dfs = self.get_smaller_dfs(data)\n",
    "        for i in range(len(smaller_dfs)):\n",
    "             \n",
    "            row = smaller_dfs[i]\n",
    "\n",
    "            print(\"before transformation: \")\n",
    "            print(row)\n",
    "\n",
    "            if not transformed:\n",
    "                row = self.transform_data(row)\n",
    "            \n",
    "            print(\"After: \")\n",
    "            print(row)\n",
    "            \n",
    "            print(\"predicting 48 hours\")\n",
    "            prediction= self.predict_48_hours(row)\n",
    "\n",
    "            preds += prediction\n",
    "\n",
    "            print(preds)\n",
    "            print(\"\\n\\n\\n\")\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    def predict_48_hours(self,row):\n",
    "        preds = []\n",
    "        for i in range(self.NUM_OF_MODELS):\n",
    "\n",
    "            x_columns = self.x_cols(i)\n",
    "\n",
    "            valid_x = row[x_columns]\n",
    "\n",
    "            current_model = self.models[i]\n",
    "            prediction = current_model.predict(valid_x)\n",
    "\n",
    "            preds += prediction.tolist()\n",
    "\n",
    "        self.save_predictions_to_db(preds, row)\n",
    "                \n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def predict_48_hours_validation(self,row, once=True):\n",
    "        aes, apes, ses = [], [], []  \n",
    "        preds = []\n",
    "        actuals = []\n",
    "        start_datetime = row.index.to_pydatetime()[0]\n",
    "        for i in range(self.NUM_OF_MODELS):\n",
    "\n",
    "            x_columns = self.x_cols(i)\n",
    "            y_columns = self.y_cols(i)\n",
    "\n",
    "            valid_x = row[x_columns]\n",
    "            valid_y = row[y_columns].values\n",
    "\n",
    "            current_model = self.models[i]\n",
    "            prediction = current_model.predict(valid_x)\n",
    "            preds += prediction.tolist()\n",
    "            actuals += list(valid_y)\n",
    "            \n",
    "            aes += list(self.absolute_errors(valid_y, prediction))\n",
    "            apes += list(self.absolute_perc_errors(valid_y, prediction))\n",
    "            ses += list((self.squared_errors(valid_y, prediction)))\n",
    "            \n",
    "        mae = self.get_mae(aes)\n",
    "        mape = self.get_mape(apes)\n",
    "        rmse = self.get_rmse(ses)\n",
    "\n",
    "        self.save_valid_data(row, actuals, preds)\n",
    "        \n",
    "        if not once:\n",
    "            return preds, actuals, aes, apes, ses\n",
    "        \n",
    "        return preds, actuals, mae, mape, rmse\n",
    "    \n",
    "        \n",
    "    def fit(self,data, transformed=True):\n",
    "        if not transformed:\n",
    "            data = self.transform_data_valid(data)\n",
    "#         models_mp = joblib.Parallel(n_jobs=8)(\n",
    "#     joblib.delayed(self.fit_on_model)(data, model_index) for model_index in range(self.NUM_OF_MODELS)\n",
    "# )\n",
    "        for i in range(self.NUM_OF_MODELS):\n",
    "            self.fit_on_model(data, i)\n",
    "\n",
    "        print(\"fitted\")\n",
    "\n",
    "            \n",
    "    def fit_on_model(self, data, i):\n",
    "        x_columns = self.x_cols(i)\n",
    "        y_columns = self.y_cols(i)\n",
    "\n",
    "        train_x = data[x_columns]\n",
    "        train_y = data[y_columns]\n",
    "\n",
    "        current_model = self.models[i]\n",
    "        current_model.fit(train_x, train_y)\n",
    "        \n",
    "            \n",
    "    def save_predictions_to_db(self, predictions, row):\n",
    "        timestep = timedelta(hours=1)\n",
    "\n",
    "        current_datetime = row.index.to_pydatetime()[0]\n",
    "\n",
    "        print(f\"number of predictions: {len(predictions)}\")\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "\n",
    "            print(f\"i: {i}\")\n",
    "            #save to predictions first:\n",
    "            load = predictions[i]  # Assuming you have a list of load values\n",
    "            # Try to get a record with the current datetime\n",
    "            record, created = Pracpredictions.objects.get_or_create(datetime=current_datetime, defaults={'load': load})\n",
    "\n",
    "            # If the record was created (datetime didn't exist), set the load value\n",
    "            if created:\n",
    "                print(\"created new prediction\")\n",
    "            else:\n",
    "                print(\"updated existing prediction\")\n",
    "\n",
    "            record.load = load\n",
    "            record.save()\n",
    "\n",
    "            # save to actuals:\n",
    "            # row/predictions should be 48 \n",
    "            # row: datetime, lags, leads\n",
    "            forecasts = self.get_lead_forecast_names(i)\n",
    "            date_decomp = self.get_date_decomp(current_datetime)\n",
    "            actuals = [current_datetime] + [load] + row[forecasts].values.tolist()[0] + date_decomp\n",
    "\n",
    "            print(\"actuals: \")\n",
    "            print(actuals)\n",
    "            \n",
    "            datetime, load, pressure, temperature, cloud_cover, wind_direction, wind_speed, date, month, hour, type_of_day = actuals\n",
    "\n",
    "            record, created = Actuals.objects.get_or_create(datetime=datetime)\n",
    "            if created:\n",
    "                print(\"created new record\")\n",
    "            else:\n",
    "                # If the record already exists, update the values\n",
    "                print(\"updated existing record\")\n",
    "\n",
    "            record.load = load\n",
    "            record.pressure = pressure\n",
    "            record.temperature = temperature\n",
    "            record.cloud_cover = cloud_cover\n",
    "            record.wind_direction = wind_direction\n",
    "            record.wind_speed = wind_speed\n",
    "            record.date = date\n",
    "            record.month = month\n",
    "            record.hour = hour\n",
    "            record.type_of_day = type_of_day\n",
    "            record.save()\n",
    "\n",
    "\n",
    "            print(\"saved to actuals\")\n",
    "            # Increment the datetime by an hour for the next iteration\n",
    "            current_datetime += timestep\n",
    "\n",
    "\n",
    "    def save_valid_data(self, row, actuals, preds):\n",
    "        current_datetime = row.index.to_pydatetime()[0]\n",
    "        timestep = timedelta(hours=1)\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            print(f\"i: {i}\")\n",
    "            prediction = preds[i]\n",
    "\n",
    "            print(f\"prediction: {prediction}\")\n",
    "            record, created = Pracpredictions.objects.get_or_create(datetime=current_datetime, defaults={'load': prediction})\n",
    "\n",
    "            if created:\n",
    "                print(\"created new prediction\")\n",
    "            else:\n",
    "                print(\"updated existing prediction\")\n",
    "\n",
    "            record.load = prediction\n",
    "            record.save()\n",
    "\n",
    "            actual = actuals[i]\n",
    "            forecasts = self.get_lead_forecast_names(i)\n",
    "            date_decomp = self.get_date_decomp(current_datetime)\n",
    "            actual_row = [current_datetime] + [actual] + row[forecasts].values.tolist()[0] + date_decomp\n",
    "\n",
    "            print(\"actuals: \")\n",
    "            print(actuals)\n",
    "            \n",
    "            datetime, actual, pressure, temperature, cloud_cover, wind_direction, wind_speed, date, month, hour, type_of_day = actual_row\n",
    "\n",
    "            record, created = Actuals.objects.get_or_create(datetime=datetime)\n",
    "            if created:\n",
    "                print(\"created new record\")\n",
    "            else:\n",
    "                # If the record already exists, update the values\n",
    "                print(\"updated existing record\")\n",
    "\n",
    "            record.load = actual\n",
    "            record.pressure = pressure\n",
    "            record.temperature = temperature\n",
    "            record.cloud_cover = cloud_cover\n",
    "            record.wind_direction = wind_direction\n",
    "            record.wind_speed = wind_speed\n",
    "            record.date = date\n",
    "            record.month = month\n",
    "            record.hour = hour\n",
    "            record.type_of_day = type_of_day\n",
    "            record.save()\n",
    "\n",
    "            print(\"saved to actuals\")\n",
    "            # Increment the datetime by an hour for the next iteration\n",
    "            current_datetime += timestep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    def add_lags(self, df):\n",
    "\n",
    "        #because the user may request to re-predict on dates that are already predicted on\n",
    "        last_date_db = Actuals.objects.latest('datetime').datetime\n",
    "        last_date_df = df['datetime'].min()\n",
    "        # if last_date_db > last_date_df:\n",
    "        #     current_date = last_date_df\n",
    "        # else:\n",
    "        #     current_date = last_date_db\n",
    "        current_date = last_date_df\n",
    "        end_date = current_date - timedelta(hours=1)\n",
    "        start_date = current_date - timedelta(days=7)\n",
    "\n",
    "        rows = Actuals.objects.filter(datetime__range=(start_date, end_date))\n",
    "        db_data = pd.DataFrame(list(rows.values())).drop(columns=['id'])\n",
    "\n",
    "        full_df = pd.concat([db_data, df]).sort_values(by='datetime')\n",
    "\n",
    "        full_df = full_df.reset_index(drop=True)\n",
    "        full_df = self.sort_df(full_df)\n",
    "\n",
    "        print(\"lags: \")\n",
    "        print(full_df)\n",
    "\n",
    "        return full_df\n",
    "\n",
    "    def get_date_decomp(self, datetime):\n",
    "        date = datetime.date()\n",
    "        month = datetime.month\n",
    "        hour = datetime.hour\n",
    "        type_of_day = datetime.isoweekday()\n",
    "\n",
    "        return [date, month, hour, type_of_day]\n",
    "\n",
    "\n",
    "    def transform_data(self, data):\n",
    "\n",
    "        #add the lags from the db\n",
    "        data = self.add_lags(data)\n",
    "\n",
    "        data = self.change_column_datatypes(data)\n",
    "\n",
    "        #cyclical encoding first\n",
    "        data = self.encode(data, 'hour', 24)\n",
    "        data = self.encode(data, 'month', 12)\n",
    "        data = self.encode(data, 'type_of_day', 31)\n",
    "\n",
    "        data = self.remove_extras(data)\n",
    "\n",
    "        data = self.get_lag_lead(data)\n",
    "\n",
    "        #get 8 am hour rows\n",
    "        data = self.condense_data(data)\n",
    "\n",
    "        # set datetime as index:\n",
    "        data = self.set_datetime_index(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "    def transform_data_valid(self, data):\n",
    "\n",
    "        #cyclical encoding first\n",
    "        data = self.encode(data, 'hour', 24)\n",
    "        data = self.encode(data, 'month', 12)\n",
    "        data = self.encode(data, 'type_of_day', 31)\n",
    "    \n",
    "        data = self.remove_extras(data)      \n",
    "\n",
    "        #get the lags and leads\n",
    "        data = self.get_lag_lead_for_valid(data)\n",
    "        \n",
    "        #get 8 am hour rows\n",
    "        data = self.condense_data_valid(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def absolute_errors(self, actuals, predicted):\n",
    "        ae = np.abs(actuals-predicted)\n",
    "        return ae\n",
    "\n",
    "    def squared_errors(self, actuals, predicted):\n",
    "        se = np.square(actuals-predicted )\n",
    "        return se\n",
    "\n",
    "    def absolute_perc_errors(self, actuals, predicted):\n",
    "        ape = ((np.abs(actuals-predicted))/ actuals) * 100\n",
    "        return ape\n",
    "    \n",
    "    def get_mae(self, aes):\n",
    "        return np.mean(aes)\n",
    "\n",
    "    def get_rmse(self, ses):\n",
    "        return np.sqrt(np.mean(ses))\n",
    "\n",
    "    def get_mape(self, apes):\n",
    "        return np.mean(apes)\n",
    "\n",
    "    #get specific lead forecast column names from whole df for each model\n",
    "    def get_lead_forecast_names(self, i):\n",
    "        cols = self.exog\n",
    "        lead_cols = []\n",
    "        for c in cols: \n",
    "            lead_cols.append(f'{c}_lead_{i}')\n",
    "        return lead_cols\n",
    "\n",
    "\n",
    "    def x_cols(self, i):\n",
    "        # Specify the text file\n",
    "        word_list = self.read_file('train_cols.txt')\n",
    "\n",
    "        word_list += self.get_lead_forecast_names(i)\n",
    "\n",
    "        return word_list\n",
    "\n",
    "\n",
    "    def y_cols(self, i):\n",
    "        return f'load_lead_{i}'\n",
    "\n",
    "\n",
    "    def split_data(self, data, test_size=0.1):\n",
    "        return np.split(data, [int((1 - test_size) * data.shape[0]) + 1])  \n",
    "    \n",
    "\n",
    "    def condense_data_valid(self, data):\n",
    "        desired_start_time = '08:00:00'\n",
    "        desired_end_time = '08:00:00'\n",
    "        start_datetime = pd.to_datetime(desired_start_time)\n",
    "        end_datetime = pd.to_datetime(desired_end_time)\n",
    "        data = data.between_time(start_datetime.time(), end_datetime.time())\n",
    "        return data\n",
    "        \n",
    "\n",
    "    def condense_data(self, data):\n",
    "        desired_start_time = '08:00:00'\n",
    "        desired_datetime = pd.to_datetime(desired_start_time).time()\n",
    "        filtered_df = data[data['datetime'].apply(lambda x: pd.to_datetime(x).time() == desired_datetime)]\n",
    "        return filtered_df\n",
    "    \n",
    "\n",
    "    def remove_extras(self, data):\n",
    "        to_remove = ['date','month', 'hour', 'type_of_day', 'date']\n",
    "        data = data.drop(to_remove, axis=1)\n",
    "        data = data.rename(columns = {'type_of_day_sin': 'day_sin', \n",
    "                            'type_of_day_cos': 'day_cos'})\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def get_lag_lead_for_valid(self, data, num_lags=168, forward_pred=48): \n",
    "        cols = self.exog + ['load']\n",
    "        #for load\n",
    "        for i in range(1, self.NUM_OF_LAGS + 1):\n",
    "            data[f'load_lag_{i}'] = data['load'].shift(i) \n",
    "\n",
    "        #for lagged weather variables\n",
    "        for c in self.exog:\n",
    "            for i in range(1, self.NUM_OF_LAGS+1):\n",
    "                data[f'{c}_lag_{i}'] = data[c].shift(i)\n",
    "\n",
    "        for i in range(0, self. NUM_OF_STEPS):    \n",
    "            data[f'load_lead_{i}'] = data['load'].shift(-i)\n",
    "\n",
    "        #weather\n",
    "        for c in self.exog:\n",
    "            for i in range(0,self. NUM_OF_STEPS):\n",
    "                data[f'{c}_lead_{i}'] = data[c].shift(-i)\n",
    "\n",
    "\n",
    "        data.dropna(inplace = True)    #drop nulls\n",
    "        data = data.set_index('datetime')   #set index as datetime\n",
    "        data = data.asfreq('H')\n",
    "\n",
    "        data = data.drop(cols, axis=1)\n",
    "\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def get_lag_lead(self, data, num_lags=168, forward_pred=48): \n",
    "        cols = self.exog\n",
    "        to_remove = cols + ['load']\n",
    "        #for lagged load\n",
    "        for i in range(1, self.NUM_OF_LAGS + 1):\n",
    "            data[f'load_lag_{i}'] = data['load'].shift(i) \n",
    "\n",
    "        #for lagged weather variables\n",
    "        for c in self.exog:\n",
    "            for i in range(1, self.NUM_OF_LAGS+1):\n",
    "                data[f'{c}_lag_{i}'] = data[c].shift(i)\n",
    "\n",
    "\n",
    "        #weather\n",
    "        for c in self.exog:\n",
    "            for i in range(0,self.NUM_OF_STEPS):\n",
    "                data[f'{c}_lead_{i}'] = data[c].shift(-i)\n",
    "\n",
    "        data = data.drop(to_remove, axis=1)\n",
    "     \n",
    "        data.dropna(inplace = True)    #drop nulls\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def set_datetime_index(self, data):\n",
    "        data = data.set_index('datetime')   #set index as datetime\n",
    "        data = data.asfreq('H')\n",
    "        return data\n",
    "\n",
    "\n",
    "    def encode(self, data, col, max_val):\n",
    "        data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "        data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def get_smaller_dfs(self, df):\n",
    "        sub_dataframes = []\n",
    "        chunk_size = self.NUM_OF_STEPS\n",
    "        step_size = chunk_size // 2\n",
    "\n",
    "        # Iterate through the original DataFrame and split it into chunks\n",
    "        for i in range(0, len(df) - chunk_size + 1, step_size):\n",
    "            chunk = df.iloc[i:i + chunk_size]\n",
    "            \n",
    "            # Check if the chunk size is equal to the specified size (48 rows)\n",
    "            if len(chunk) == chunk_size:\n",
    "                sub_dataframes.append(chunk)\n",
    "        return sub_dataframes\n",
    "\n",
    "\n",
    "    def change_column_datatypes(self, df):\n",
    "\n",
    "        data_types = self.read_json_file('colTypes.json')\n",
    "\n",
    "        for column, dtype in data_types.items():\n",
    "            try:\n",
    "                # Attempt to convert the column to the desired data type\n",
    "                df[column] = df[column].astype(dtype)\n",
    "\n",
    "            except (ValueError, TypeError, KeyError):\n",
    "                # Handle any potential errors (e.g., if conversion is not possible)\n",
    "                response = f\"Error converting '{column}' to {dtype}\"\n",
    "                valid = False\n",
    "            \n",
    "        return df\n",
    "\n",
    "    \n",
    "    def read_file(self, filename):\n",
    "        # Initialize an empty list to store the words\n",
    "        word_list = []\n",
    "        # Open the file and read its contents line by line\n",
    "        filepath = os.path.join(self.path, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                # Remove leading and trailing whitespace and append the word to the list\n",
    "                word_list.append(line.strip())\n",
    "        return word_list\n",
    "    \n",
    "\n",
    "\n",
    "    def read_json_file(self, filename):\n",
    "        filename = os.path.join(self.path, filename)\n",
    "        with open(filename, \"r\") as json_file:\n",
    "            file = json.load(json_file)\n",
    "        return file\n",
    "    \n",
    "    def save_model(self):\n",
    "        filename = \"ElectricityDemandForecaster.joblib\"\n",
    "        filepath = os.path.join(self.path, filename)\n",
    "        joblib.dump(self, filepath)\n",
    "\n",
    "    \n",
    "    def sort_df(self, df):\n",
    "        df = df.sort_values(by='datetime', ascending=True)\n",
    "        return df\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
